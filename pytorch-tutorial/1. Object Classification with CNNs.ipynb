{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Classification with CNNs\n",
    "\n",
    "Object classification in images with convolutional neural networks (CNNs) is arguably what ignited the field of deep learning (or re-ignited the field of neural networks). The structural prior given by using convolutions is incredibly powerful when processing images, such that features extracted from a randomly initialised CNN can be pretty useful.\n",
    "\n",
    "Deep learning with CNNs shine in computer vision tasks with plenty of labelled data, so we'll look at a fairly standard pipeline based on object classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, which comprises of 60,000 32x32 colour images in 10 classes. The 10 classes contain basic categories of both natural and manmade objects. Here's a selection of some from each class:\n",
    "\n",
    "![CIFAR-10 samples](https://kaggle2.blob.core.windows.net/competitions/kaggle/3649/media/cifar-10.png)\n",
    "\n",
    "The dataset is split into 50,000 training images and 10,000 test images, with a uniform distribution of examples per class. We can use a premade dataset loader from the `torchvision` package. We'll also normalise the data using mean and standard deviation statistics pre-calculated from the entire dataset. When loading training data, we'll also apply some (class-preserving) data augmentations in order to make the model more robust to these sorts of transformations. Finally, we'll make use of asynchronous data loading workers to queue up data while the network is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.display import clear_output, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data_path = os.path.join(os.path.expanduser('~'), '.torch', 'datasets', 'cifar10')\n",
    "train_data = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "]))\n",
    "test_data = datasets.CIFAR10(data_path, train=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For the model we'll use a fairly standard set of components - (strided) convolutions, batch normalisation, dropout and ELU nonlinearities. This is on the small side and not well-tuned for CIFAR-10, but is just supposed to be representative of a typical CNN (without skip connections, which are commonplace in deeper architectures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 7, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512, bias=False)\n",
    "        self.dp1 = nn.Dropout()\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "        x = F.elu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.elu(self.bn5(self.dp1(self.fc1(x))))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "We'll instantiate the model here and train it for a few epochs and plot the training loss for each minibatch. At the end of each epoch we'll also include the test loss, calculated over the entire test set. Finally, we'll print the classification accuracy of the trained model on the test set. It's important to use `.train()` and `.eval()` on the model as appropriate, as batch normalisation and dropout should perform differently depending on whether the model is training or is being used for evaluation. During testing, using the `torch.no_grad()` context manager prevents a computation graph being stored (as otherwise, PyTorch would store a graph because the data interacts with parameters that require gradients by default).\n",
    "\n",
    "Note that to get good performance on this sort of dataset, you would typically want a deeper model running on GPU for many epochs. However, the general components of the architecture, or the training pipeline with data augmentation and normalisation, would remain much the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimiser = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "train_losses, test_losses, test_acc = [], [], 0\n",
    "epochs, iters_per_epoch = 3, len(train_loader)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plotted_legend = False\n",
    "\n",
    "def plot():\n",
    "    global plotted_legend\n",
    "    plt.plot(range(len(train_losses)), train_losses, 'b-', label='Train')\n",
    "    plt.plot([(i + 1) * iters_per_epoch - 1 for i in range(len(test_losses))], test_losses, 'r-', label='Test')\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    if not plotted_legend:\n",
    "        plt.legend(loc='upper right')\n",
    "        plotted_legend = True\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimiser.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        optimiser.step()\n",
    "        if i % 10 == 0:\n",
    "            plot()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            y_hat = model(x)\n",
    "            test_loss += F.cross_entropy(y_hat, y, reduction='sum').item()\n",
    "            pred = y_hat.argmax(1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_losses.append(test_loss / len(test_data))\n",
    "    return correct / len(test_data)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train()\n",
    "    test_acc = test()\n",
    "plot()\n",
    "clear_output(wait=True)\n",
    "display('Final test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration\n",
    "\n",
    "We can get over 50% accuracy in not too long with a small CNN running on CPU. However, most NNs in use nowadays benefit from the parallelisation afforded by GPUs. We'll make a larger network (with a combination of residual blocks and grouped convolutions that goes by the name of ResNeXt) which would take longer to train on CPU, and run it on GPU. We'll also demonstrate using a learning rate scheduler (with a non-standard schedule), as these are often used when really optimising for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.prelu1 = nn.PReLU(2 * hidden_size)\n",
    "        self.conv1 = nn.Conv2d(2 * hidden_size, hidden_size, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_size)\n",
    "        self.prelu2 = nn.PReLU(hidden_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, padding=1, groups=hidden_size // 4, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_size)\n",
    "        self.prelu3 = nn.PReLU(hidden_size)\n",
    "        self.conv3 = nn.Conv2d(hidden_size, 2 * hidden_size, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.bn1(self.conv1(self.prelu1(x)))\n",
    "        r = self.bn2(self.conv2(self.prelu2(r)))\n",
    "        r = self.bn3(self.conv3(self.prelu3(r)))\n",
    "        x = r + x\n",
    "        return x\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 7, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.prelu1 = nn.PReLU(32)\n",
    "        self.res1 = ResNeXtBlock(16)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.prelu2 = nn.PReLU(64)\n",
    "        self.res2 = ResNeXtBlock(32)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.prelu3 = nn.PReLU(128)\n",
    "        self.res3 = ResNeXtBlock(64)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.prelu4 = nn.PReLU(256)\n",
    "        self.fc1 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.bn1(self.conv1(x)))\n",
    "        x = self.prelu2(self.bn2(self.conv2(self.res1(x))))\n",
    "        x = self.prelu3(self.bn3(self.conv3(self.res2(x))))\n",
    "        x = self.prelu4(self.bn4(self.conv4(self.res3(x))))\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1)).squeeze()\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNeXt().cuda()\n",
    "optimiser = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimiser, lr_lambda=lambda epoch: 5 if epoch == 1 else 1)\n",
    "train_losses, test_losses, test_acc = [], [], 0\n",
    "epochs, iters_per_epoch = 3, len(train_loader)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plotted_legend = False\n",
    "\n",
    "def plot():\n",
    "    global plotted_legend\n",
    "    plt.plot(range(len(train_losses)), train_losses, 'b-', label='Train')\n",
    "    plt.plot([(i + 1) * iters_per_epoch - 1 for i in range(len(test_losses))], test_losses, 'r-', label='Test')\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    if not plotted_legend:\n",
    "        plt.legend(loc='upper right')\n",
    "        plotted_legend = True\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimiser.zero_grad()\n",
    "        y_hat = model(x.cuda())\n",
    "        loss = F.cross_entropy(y_hat, y.cuda())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        optimiser.step()\n",
    "        if i % 10 == 0:\n",
    "            plot()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            y_hat = model(x.cuda())\n",
    "            test_loss += F.cross_entropy(y_hat, y.cuda(), reduction='sum').item()\n",
    "            pred = y_hat.argmax(1, keepdim=True).cpu()\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_losses.append(test_loss / len(test_data))\n",
    "    return correct / len(test_data)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    scheduler.step()\n",
    "    train()\n",
    "    test_acc = test()\n",
    "plot()\n",
    "clear_output(wait=True)\n",
    "display('Final test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
