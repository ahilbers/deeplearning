{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartpole:\n",
    "    def __init__(self, theta, gamma, train_param):\n",
    "        self.theta = theta\n",
    "        self.gamma = gamma\n",
    "        self.train_param = train_param\n",
    "        \n",
    "    def decaysum(self, rewards):\n",
    "        \"\"\"Calculate sum of future rewards with decay factor gamma\"\"\"\n",
    "        length = rewards.shape[0]\n",
    "        weights = (self.gamma * np.ones(length)) ** np.arange(length)\n",
    "        weightedsum = np.dot(rewards, weights)\n",
    "        return weightedsum\n",
    "    \n",
    "    def action_simple(self, observation):\n",
    "        \"\"\"Move left if pole is leaning left and right otherwise\"\"\"\n",
    "        if observation[2] < 0: return 0\n",
    "        else: return 1\n",
    "        \n",
    "    def action_sigmoid(self, observation, theta):\n",
    "        \"\"\"Move left or right based on a sigmoid function\"\"\"\n",
    "        sigmoid = 1 / (1 + np.exp(-np.dot(observation, theta)))\n",
    "        # Go right (1) with probability sigmoid, else go left (0)\n",
    "        rand_cf = np.random.rand(1)\n",
    "        if rand_cf < sigmoid: return 1\n",
    "        else: return 0 \n",
    "    \n",
    "    def update_theta(self, theta, actions, states, fwd_rewards):\n",
    "        \"\"\"Find the direction of gradient descent to update theta\"\"\"\n",
    "        num_steps = actions.shape[0]\n",
    "        update = np.zeros(4)\n",
    "        for step in range(num_steps):\n",
    "            factor1 = int(actions[step]==0) - int(actions[step]==1)\n",
    "            factor2 = 1 - 1/(1+np.exp(-np.dot(observation, theta)))\n",
    "            update = update + factor1 * factor2 * states[step]\n",
    "        return update\n",
    "    \n",
    "    def run_simulations(self, num_episodes, episode_length):\n",
    "        \"\"\"Run simulations and find direction in which to update theta\"\"\"\n",
    "        \n",
    "        theta = self.theta\n",
    "        update_all = np.zeros(4)\n",
    "        train_param = self.train_param\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        # Shows the number of steps until the pole falls. If the\n",
    "        # pole does not fall, we want it to give the episode_length\n",
    "        success_array = episode_length * np.ones(num_episodes)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            observation = env.reset()\n",
    "            \n",
    "            # Keep log of the states, actions, and rewards\n",
    "            states = np.zeros(shape=(episode_length, 4))\n",
    "            actions = np.zeros(shape=(episode_length))\n",
    "            rewards = np.zeros(shape=(episode_length))\n",
    "    \n",
    "            # Run each episode and record relevant data\n",
    "            for step in range(episode_length):\n",
    "                states[step] = observation\n",
    "                action = self.action_sigmoid(observation, theta)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                actions[step] = action\n",
    "                rewards[step] = reward\n",
    "                \n",
    "                # Finish the loop if pole has tilted too far\n",
    "                if done:\n",
    "                    success_array[episode] = step + 1\n",
    "                    break\n",
    "                \n",
    "            # Input the total (decaying) future rewards\n",
    "            fwd_rewards = np.zeros(shape=(episode_length))\n",
    "            for step in range(episode_length):\n",
    "                fwd_rewards[step] = self.decaysum(rewards[step:])\n",
    "            \n",
    "            update = self.update_theta(theta, actions, states, fwd_rewards)\n",
    "            update_all = update_all + update\n",
    "\n",
    "        print(np.mean(success_array))\n",
    "        print(update_all)\n",
    "        print(theta + train_param * update_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0.50, 22.76, -2.16, -40.25]\n",
    "gamma = 0.9\n",
    "train_param = 0.001\n",
    "model = cartpole(theta=theta, gamma=gamma, \\\n",
    "        train_param=train_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3008\n",
      "[ -2585.37254602 -51735.36879633   3741.30504894  79175.82563172]\n",
      "[ -2.08537255 -28.9753688    1.58130505  38.92582563]\n"
     ]
    }
   ],
   "source": [
    "model.run_simulations(10000, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaysum(rewards, gamma):\n",
    "    \"\"\"Calculate sum of future rewards with decay factor gamma\"\"\"\n",
    "    length = rewards.shape[0]\n",
    "    weights = (gamma * np.ones(length)) ** np.arange(length)\n",
    "    weightedsum = np.dot(rewards, weights)\n",
    "    return weightedsum\n",
    "        \n",
    "def action_sigmoid(observation, theta):\n",
    "    \"\"\"Move left or right based on a sigmoid function\"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-np.dot(observation, theta)))\n",
    "    # Go right (1) with probability sigmoid, else go left (0)\n",
    "    rand_cf = np.random.rand(1)\n",
    "    if rand_cf < sigmoid: return 1\n",
    "    else: return 0 \n",
    "    \n",
    "def update_theta(theta, actions, states, fwd_rewards):\n",
    "    \"\"\"Find the direction of gradient descent to update theta\"\"\"\n",
    "    num_steps = actions.shape[0]\n",
    "    update = np.zeros(4)\n",
    "    for step in range(num_steps):\n",
    "        factor1 = int(actions[step]==0) - int(actions[step]==1)\n",
    "        factor2 = 1 - 1/(1+np.exp(-np.dot(observation, theta)))\n",
    "        update = update + factor1 * factor2 * states[step]\n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.259\n",
      "[  8.61513744  53.37364918 -10.21788828 -78.59133177]\n",
      "[ 0.00861514  0.05337365 -0.01021789 -0.07859133]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "episode_length = 250\n",
    "train_param = 0.001     # Training parameter\n",
    "gamma = 0.9    # Decay rate\n",
    "\n",
    "theta = np.array([0., 0., 0., 0.])\n",
    "#theta = np.array([-0.8, 3.7, 0.7, 6.8])\n",
    "update_all = np.zeros(4)\n",
    "\n",
    "# Shows the number of steps until the pole falls. If the\n",
    "# pole does not fall, we want it to give the episode_length\n",
    "success_array = episode_length * np.ones(num_episodes)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # Keep a log of the states, actions rewards and future \n",
    "    # weighted decay rewards\n",
    "    states = np.zeros(shape=(episode_length, 4))\n",
    "    actions = np.zeros(shape=(episode_length))\n",
    "    rewards = np.zeros(shape=(episode_length))\n",
    "    \n",
    "    # Do 100 steps, take a step and record all relevant data\n",
    "    for step in range(episode_length):\n",
    "        states[step] = observation\n",
    "        action = action_sigmoid(observation, theta)\n",
    "        actions[step] = action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewards[step] = reward\n",
    "        \n",
    "        # Finish the loop if the pole has tilted too far\n",
    "        if done:\n",
    "            success_array[episode] = step + 1\n",
    "            break\n",
    "            \n",
    "    # Input the total (decaying) future rewards\n",
    "    fwd_rewards = np.zeros(shape=(episode_length))\n",
    "    for step in range(episode_length):\n",
    "        fwd_rewards[step] = decaysum(rewards[step:], gamma)\n",
    "        \n",
    "    update = update_theta(theta, actions, states, fwd_rewards)\n",
    "    update_all = update_all + update\n",
    "\n",
    "print(np.mean(success_array))\n",
    "print(update_all)\n",
    "print(theta + train_param * update_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Results over 10000 episodes:\n",
    "\n",
    "Action                     Mean time until done\n",
    "constant (0 or 1)          9.35\n",
    "action_simple              42.17\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.placeholder(tf.float32, shape=(4,))\n",
    "state = tf.placeholder(tf.float32, shape=(4,))\n",
    "dot = tf.tensordot(theta, state, axes=1)\n",
    "sigmoid = tf.nn.sigmoid(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    feed_dict = {theta: np.array([1, 1, 1, 1]), \\\n",
    "                 state: np.array([1, 1, 1, 1])}\n",
    "    out = sess.run(sigmoid, feed_dict=feed_dict)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
