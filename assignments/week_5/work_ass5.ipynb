{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartpole:\n",
    "    def __init__(self, theta, gamma, train_param):\n",
    "        self.theta = theta\n",
    "        self.gamma = gamma\n",
    "        self.train_param = train_param\n",
    "        \n",
    "    def decaysum(self, rewards):\n",
    "        \"\"\"Calculate sum of future rewards with decay factor gamma\"\"\"\n",
    "        length = rewards.shape[0]\n",
    "        weights = (self.gamma * np.ones(length)) ** np.arange(length)\n",
    "        weightedsum = np.dot(rewards, weights)\n",
    "        return weightedsum\n",
    "    \n",
    "    def action_simple(self, observation):\n",
    "        \"\"\"Move left if pole is leaning left and right otherwise\"\"\"\n",
    "        if observation[2] < 0: return 0\n",
    "        else: return 1\n",
    "        \n",
    "    def action_sigmoid(self, observation, theta):\n",
    "        \"\"\"Move left or right based on a sigmoid function\"\"\"\n",
    "        sigmoid = 1 / (1 + np.exp(-np.dot(observation, theta)))\n",
    "        # Go right (1) with probability sigmoid, else go left (0)\n",
    "        rand_cf = np.random.rand(1)\n",
    "        if rand_cf < sigmoid: return 1\n",
    "        else: return 0 \n",
    "    \n",
    "    def update_theta(self, theta, actions, states, fwd_rewards):\n",
    "        \"\"\"Find the direction of gradient descent to update theta\"\"\"\n",
    "        num_steps = actions.shape[0]\n",
    "        update = np.zeros(4)\n",
    "        for step in range(num_steps):\n",
    "            factor1 = int(actions[step]==0) - int(actions[step]==1)\n",
    "            factor2 = 1 - 1/(1+np.exp(-np.dot(observation, theta)))\n",
    "            update = update + factor1 * factor2 * states[step]\n",
    "        return update\n",
    "    \n",
    "    def run_simulations(self, num_episodes, episode_length):\n",
    "        \"\"\"Run simulations and find direction in which to update theta\"\"\"\n",
    "        \n",
    "        theta = self.theta\n",
    "        update_all = np.zeros(4)\n",
    "        train_param = self.train_param\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        #Â Shows the number of steps until the pole falls. If the\n",
    "        # pole does not fall, we want it to give the episode_length\n",
    "        success_array = episode_length * np.ones(num_episodes)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            observation = env.reset()\n",
    "            \n",
    "            # Keep log of the states, actions, and rewards\n",
    "            states = np.zeros(shape=(episode_length, 4))\n",
    "            actions = np.zeros(shape=(episode_length))\n",
    "            rewards = np.zeros(shape=(episode_length))\n",
    "    \n",
    "            # Run each episode and record relevant data\n",
    "            for step in range(episode_length):\n",
    "                states[step] = observation\n",
    "                action = self.action_sigmoid(observation, theta)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                actions[step] = action\n",
    "                rewards[step] = reward\n",
    "                \n",
    "                # Finish the loop if pole has tilted too far\n",
    "                if done:\n",
    "                    success_array[episode] = step + 1\n",
    "                    break\n",
    "                \n",
    "            # Input the total (decaying) future rewards\n",
    "            fwd_rewards = np.zeros(shape=(episode_length))\n",
    "            for step in range(episode_length):\n",
    "                fwd_rewards[step] = self.decaysum(rewards[step:])\n",
    "            \n",
    "            update = self.update_theta(theta, actions, states, fwd_rewards)\n",
    "            update_all = update_all + update\n",
    "\n",
    "        print(np.mean(success_array))\n",
    "        print(update_all)\n",
    "        print(theta + train_param * update_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
