{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model class in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehot(label):\n",
    "    \"\"\"Change numbers 0-15 to unit vectors along the first 16 axes.\"\"\"\n",
    "    \n",
    "    label_onehot = np.zeros(shape=(16))\n",
    "    label_onehot[label] = 1.0\n",
    "    \n",
    "    return label_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLake():\n",
    "    def __init__(self, gamma, learn_param, epsilon):\n",
    "        self.gamma = gamma\n",
    "        self.learn_param = learn_param\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # State is one-hot vector, 16 possible locations\n",
    "        self.state_oh = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "        self.sess = tf.Session()\n",
    "        self._build_graph()\n",
    "        self.sess.run(tf.initializers.global_variables())\n",
    "        \n",
    "        \n",
    "    def _build_graph(self):\n",
    "        \"\"\"At each state, there are 4 possible actions. The predicted optimal action\n",
    "        is to the state with the highest Q value\"\"\"\n",
    "        self.weights = tf.Variable(tf.random_uniform([16, 4], minval=0, maxval=0.1))\n",
    "        self.Q1 = tf.matmul(self.state_oh, self.weights)\n",
    "        self.prediction = tf.argmax(self.Q1, axis=1)\n",
    "            \n",
    "        \"\"\"Construct the graph for Q learning\"\"\"\n",
    "        self.Q2 = tf.placeholder(shape=[1, 4], dtype=tf.float32)\n",
    "        loss = tf.reduce_sum(tf.square(self.Q2 - self.Q1))\n",
    "        gdo = tf.train.GradientDescentOptimizer(learning_rate = self.learn_param)\n",
    "        self.updatedweights = gdo.minimize(loss)       \n",
    "        \n",
    "    def train_model(self, num_steps, num_episodes):\n",
    "        \"\"\"Conduct reinforcement learning to optimise Q\"\"\"\n",
    "            \n",
    "        reward_total = 0\n",
    "        # success_array shows 1 if episode was successful, 0 otherwise. success_rate is \n",
    "        # proportion of last 100 runs that was successful (first 99 entries remain 0)\n",
    "        success_array = np.zeros(shape=(num_episodes))\n",
    "        success_rate = np.zeros(shape=(num_episodes))\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            # Reset experiment\n",
    "            state_curr, done, reward = env.reset(), False, 0\n",
    "                \n",
    "            info = []\n",
    "            info.append(1)\n",
    "            for step in range(num_steps):\n",
    "                    \n",
    "                # Determine best action\n",
    "                feed_dict = {self.state_oh: [make_onehot(state_curr)]}\n",
    "                action, Y = self.sess.run([self.prediction, self.Q1], feed_dict=feed_dict)\n",
    "                print('state_curr: {}'.format(state_curr))\n",
    "                    \n",
    "                # Take a step. Do random action with probability epsilon, where epsilon \n",
    "                # diminishes over time (less and less exploration)\n",
    "#                if self.epsilon > np.random.rand(1):\n",
    "#                    action[0] = env.action_space.sample()\n",
    "#                    self.epsilon -= 10**-3\n",
    "                state_next, reward, done, _ = env.step(action[0])\n",
    "    \n",
    "                print('action: {}'.format(action[0]))\n",
    "                print('reward: {}'.format(reward))\n",
    "                print('state_next: {}'.format(state_next))\n",
    "                    \n",
    "                # Evaluate expected reward from this step and calculate second term in \n",
    "                # Bellman equation iteration step\n",
    "                feed_dict = {self.state_oh: [make_onehot(state_next)]}\n",
    "                Y1 = self.sess.run(self.Q1, feed_dict=feed_dict)\n",
    "                change_Y = Y.copy()\n",
    "                change_Y[0, action[0]] = reward + gamma*np.max(Y1)\n",
    "                    \n",
    "                # Update weights by moving them slightly towards change_Y â€” this\n",
    "                # corresponds to the Bellman equation.\n",
    "                feed_dict = {self.state_oh: [make_onehot(state_next)], self.Q2: change_Y}\n",
    "                _, new_weights = \\\n",
    "                        self.sess.run([self.updatedweights,self.weights], feed_dict=feed_dict)\n",
    "                print(Y[0])\n",
    "                print(change_Y[0])\n",
    "                print(new_weights[:6])\n",
    "                    \n",
    "                # Add the reward and get ready for next iteration\n",
    "                reward_total += reward\n",
    "                state_curr = state_next\n",
    "                info.append(state_curr)\n",
    "                \n",
    "                print('')\n",
    "                \n",
    "                if done:\n",
    "                    if reward == 1: print(episode)\n",
    "                    break\n",
    "                    \n",
    "#                if reward == 1:\n",
    "#                    success_array[episode] = 1\n",
    "#                    print(episode)\n",
    "#                if episode > 98:\n",
    "#                    success_rate[episode] - np.sum(success_array[episode-99:episode+1])/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_curr: 0\n",
      "action: 3\n",
      "reward: 0.0\n",
      "state_next: 1\n",
      "[0.08663105 0.07526981 0.03512669 0.09116941]\n",
      "[0.08663105 0.07526981 0.03512669 0.06993104]\n",
      "[[0.08663105 0.07526981 0.03512669 0.09116941]\n",
      " [0.07948713 0.05724798 0.0109294  0.05573143]\n",
      " [0.00485086 0.09917449 0.01917681 0.08917377]\n",
      " [0.02985833 0.09743996 0.02228588 0.08596214]\n",
      " [0.03608974 0.09987166 0.03262559 0.06735255]\n",
      " [0.07045231 0.08531383 0.07180941 0.06034572]]\n",
      "\n",
      "state_curr: 1\n",
      "action: 0\n",
      "reward: 0.0\n",
      "state_next: 1\n",
      "[0.07948713 0.05724798 0.0109294  0.05573143]\n",
      "[0.07153842 0.05724798 0.0109294  0.05573143]\n",
      "[[0.08663105 0.07526981 0.03512669 0.09116941]\n",
      " [0.07789738 0.05724798 0.0109294  0.05573143]\n",
      " [0.00485086 0.09917449 0.01917681 0.08917377]\n",
      " [0.02985833 0.09743996 0.02228588 0.08596214]\n",
      " [0.03608974 0.09987166 0.03262559 0.06735255]\n",
      " [0.07045231 0.08531383 0.07180941 0.06034572]]\n",
      "\n",
      "state_curr: 1\n",
      "action: 0\n",
      "reward: 0.0\n",
      "state_next: 1\n",
      "[0.07789738 0.05724798 0.0109294  0.05573143]\n",
      "[0.07010765 0.05724798 0.0109294  0.05573143]\n",
      "[[0.08663105 0.07526981 0.03512669 0.09116941]\n",
      " [0.07633944 0.05724798 0.0109294  0.05573143]\n",
      " [0.00485086 0.09917449 0.01917681 0.08917377]\n",
      " [0.02985833 0.09743996 0.02228588 0.08596214]\n",
      " [0.03608974 0.09987166 0.03262559 0.06735255]\n",
      " [0.07045231 0.08531383 0.07180941 0.06034572]]\n",
      "\n",
      "state_curr: 1\n",
      "action: 0\n",
      "reward: 0.0\n",
      "state_next: 5\n",
      "[0.07633944 0.05724798 0.0109294  0.05573143]\n",
      "[0.07678245 0.05724798 0.0109294  0.05573143]\n",
      "[[0.08663105 0.07526981 0.03512669 0.09116941]\n",
      " [0.07633944 0.05724798 0.0109294  0.05573143]\n",
      " [0.00485086 0.09917449 0.01917681 0.08917377]\n",
      " [0.02985833 0.09743996 0.02228588 0.08596214]\n",
      " [0.03608974 0.09987166 0.03262559 0.06735255]\n",
      " [0.07171834 0.07970066 0.05963341 0.05942286]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "learn_param = 0.1\n",
    "epsilon = 0.1\n",
    "model = FrozenLake(gamma=gamma, learn_param=learn_param, epsilon=epsilon)\n",
    "model.train_model(num_steps=100, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
