{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates the location of the player, as a one-hot vector\n",
    "inputs = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "\n",
    "# Each of the 16 states has 4 possible outcomes\n",
    "weights = tf.Variable(tf.random_uniform([16,4], minval=0, maxval=0.1))\n",
    "Q1 = tf.matmul(inputs, weights)\n",
    "\n",
    "# Next state is chosen by maximising the dot product\n",
    "prediction = tf.argmax(Q1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the weights, by moving the row of the weights matrix\n",
    "# slightly closer to Q2. \n",
    "Q2 = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(Q2 - Q1))\n",
    "gdo = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updatedweights = gdo.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "num_episodes = 1000\n",
    "\n",
    "totalReward = 0\n",
    "\n",
    "# success_array the success of each episode. 1 if succesful, \n",
    "# 0 otherwise. success_rate shows sliding scale of last 100\n",
    "# episodes. The first 99 entries remain 0.\n",
    "success_array = np.zeros(shape=(num_episodes))\n",
    "success_rate = np.zeros(shape=(num_episodes))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.initializers.global_variables()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        # Reset the experiment: start at square 1\n",
    "        state_now = env.reset()\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        num_steps = 50\n",
    "        \n",
    "        info = np.zeros(shape=(num_steps, 5))\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            \n",
    "            # Find the estimate for the best action\n",
    "            feed_dict = {inputs: [np.eye(16)[state_now]]}\n",
    "            wants = [prediction, Q1]\n",
    "            action, Y = sess.run(wants, feed_dict=feed_dict)\n",
    "            \n",
    "            # Do a random action with probability epsilon, where epsilon\n",
    "            # diminishes over time (so less and less exploration). Eventually\n",
    "            # there is no exploration occuring anymore\n",
    "            if epsilon > np.random.rand(1):\n",
    "                action[0] = env.action_space.sample()\n",
    "#                epsilon -= 10**-3\n",
    "            \n",
    "            # Take a step\n",
    "            state_next, reward, done, _ = env.step(action[0])\n",
    "            \n",
    "            # Evaluate expected reward from this step and adjust weights\n",
    "            feed_dict = {inputs: [np.eye(16)[state_next]]}\n",
    "            wants = Q1\n",
    "            Y1 = sess.run(wants, feed_dict=feed_dict)\n",
    "            change_Y = Y\n",
    "            change_Y[0, action[0]] = reward + gamma*np.max(Y1)\n",
    "            \n",
    "            # Update the weights by moving the weights slightly in the direction\n",
    "            # of the newly found Q row. The new Q values are not immediately\n",
    "            # taken from Bellman's equations, but nudged a little bit in the\n",
    "            # correct direction\n",
    "            feed_dict = {inputs: [np.eye(16)[state_now]], \\\n",
    "                         Q2: change_Y}\n",
    "            wants = [updatedweights, weights]\n",
    "            _, new_weights = sess.run(wants, feed_dict=feed_dict)\n",
    "            \n",
    "            # Add the reward and get ready for the next iteration\n",
    "            totalReward += reward\n",
    "            state_now = state_next\n",
    "            \n",
    "            # Input the summary statistics\n",
    "            if reward == 1:\n",
    "                success_array[episode] = 1\n",
    "            if episode > 98:\n",
    "                success_rate[episode] = \\\n",
    "                        np.sum(success_array[episode-99:episode+1])/100\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(100, num_episodes), success_rate[100:])\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Proportion successful')\n",
    "    ax.set_title('Learning, exploration constant')\n",
    "    plt.savefig('constant.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
