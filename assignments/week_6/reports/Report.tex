\documentclass[]{article}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows}
\graphicspath{ {../figs/} }


% Title Page
\title{Industrial revolution 2.0: using variational autoencoders to produce images of clothing}
\author{Adriaan Hilbers}


\begin{document}
\maketitle




\section*{Abstract}
\label{sec:abstrct}

This document describes the use of variational autoencoders as generative models for images of articles of clothing. XXXXX.
This work was done as part of the \textit{Deep Learning} course at Imperial College London in Autumn 2018.




\section{Introduction}
\label{sec:introduction}

In the last few decades, machine learning techniques have achieved considerable success in many taks previously viewed as difficult for a computer. \textit{Generative models} are one specific class of algorithms that seek to produce new objects (e.g. images or music) without the need of a human. Such models are typically \textit{trained} from ``real'' data such as older images or audio fragments to infer the structures. These structures can then be imitated to create new objects that are derivations of those trained on. Various generative frameworks exist, including generative adversarial neural networks, variational autoencoders and normalising flows \cite{notes}. 

In this report, we employ a \textit{variational autoencoder} to produce new images of clothing. The dataset and model architecture are introduced in sections XX and XX respectively. Only a very brief discussion of the model architecture is provided, but the code used is publically available at \texttt{github.com/ahilbers} under \texttt{deeplearning/assignments/week\_6}. 




\section{Dataset: Fashion MNIST}
\label{sec:dataset}

To train our model we use the \textit{Fashion MNIST} dataset\footnote{see https://github.com/zalandoresearch/fashion-mnist}. It contains 70,000 images of articles of clothing, classified into one of 10 categories. The data is split into 60,000 \textit{training} images and 10,000 \textit{testing} images. An overview of the images, by category, is shown in Figure \ref{fig:mnist_fashion}.

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_0.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_1.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_2.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_3.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_4.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_5.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_6.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_7.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_8.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_9.pdf}
  \caption{100 samples images from the \textit{Fashion MNIST} dataset. Each row shows 10 examples from each clothing type: t-shirt/top, trousers, pullover, dress, coat, sandal, shirt, sneaker bag or ankle boot.}
  \label{fig:mnist_fashion}
\end{figure}

The dataset is designed to be a drop-in replacement for the traditional MNIST database of handwritten digits, with image dimensions and the number of possible labels identical. However, the \textit{Fashion MNIST} database is more challenging for machine learning algorithms, since the inter-category variations are larger and the intra-category variations smaller; the visual difference between a coat and pullover may be very small when compared to two different handwritten digits.




\section{Variational Autoencoder: setup}
\label{sec:vae}

This section provides a brief introduction into the model architecture and how training is performed. The variational autoencoder employed is inspired by the introductions to to the method given in \cite{notes, frans, altosaar, shafkat}.

The variational autoencoder employed consists of two parts: an \textit{encoder} and a \textit{deocder}. It works as follows:
\begin{itemize}
\item Represent an image by $\bm{x}^{(i)}$, the vector of its pixel brightnesses.
\item Encode this image into a mean vector $\bm{\mu}^{(i)}$ and variance vector $\bm{\sigma}^{(i)}$ with dimension $n_z$.
 \item Sample a vector $\bm{z}^{(i)}$ from $\mathcal{N}(\bm{\mu}^{(i)}, \bm{\sigma}^{(i)})$ where the covariance matrix is diagonal. In reality, a \textit{reparametrisation trick} (see e.g. \cite{notes}) is performed to ensure the model is trainable, but this does not change the results of a feedforward run through the autoencoder). The density of $\bm{z}^{(i)}$ given $\bm{x}^{(i)}$ is denoted by $q_{\phi}(\bm{y}^{(i)}|\bm{x}^{(i)})$, where $\phi$ represents all parameters in the encoder). 
 \item Decode $\bm{z}^{(i)}$ back into image space using a decoder $p_{\sigma}(\bm{x}^{(i)}|\bm{z}^{(i)})$. 
\end{itemize}

For a traditional autoencoder, the loss function will involve only the difference between $p_{\sigma}(\bm{x}^{(i)}|\bm{z}^{(i)})$ and the input $\bm{x}^{(i)}$. For a variational encoder, we want to generate new images, so this is not sufficient. The method of generating new images is by sampling new vectors $\bm{z}$ from its latent space of dimension $n_z$. If this latent space has ``meaning'' in terms of properties of the images, we will generate images. Furthemore, we hope that coordinates in the latent space are ``meaningful'' in the sense that the $\bm{z}$ vector halfway between e.g. a shoe and a trousers is some pictorial intermediate of the two.




For this to work, the distribution of encoded (noisy) $z$ values from the training dataset should be distributed in some meaningful way. We enforce this by adding a penalisation term to the loss function that measures the deviation of the distribution of $z$ values throughout the training dataset from a standard normal distribution $\mathcal{N}(0, 1)$. For this purpose, we use the Kullback-Leibler divergence $D_{KL}$.

The loss function is a follows:
\begin{align}
  \label{eq:loss}
  \mathcal{L}(\bm{\theta}, \bm{\phi}, \bm{x}^{(i)}) &= \frac{1}{2} \sum_{j=1}^d \left( 1 + \log{((\sigma_j^{(i)})^2)} - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2 \right) \\
  &+ \frac{1}{L} \sum_{l=1}^L \log{p_{\bm{\theta}}(\bm{x}^{(i)}|\bm{z}^{(i, l)})}
\end{align}

. 

\begin{figure}
  \centering
  % Define block styles
  \tikzstyle{block} = [rectangle, draw, fill=blue!20, 
      text width=12em, text centered, rounded corners, minimum height=1em]
  \tikzstyle{block_sm} = [rectangle, draw, fill=blue!20, 
      text width=4em, text centered, rounded corners, minimum height=1em]  
  \tikzstyle{line} = [draw, -latex']
    
  \begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node[draw=none,fill=none, node distance=1cm] (inp) {input image};
    \node[block, below of=inp, node distance=1cm] (conv_1) {convolutional layer};
    \node[block, below of=conv_1, node distance=1cm] (conv_2) {convolutional layer};
    \node[block, below of=conv_2, node distance=1cm] (dense_1) {dense layer};
    \node[draw=none, fill=none, below of=dense_1, node distance=1cm] (inv1) {};
    \node[draw=none, fill=none, left of=inv1, node distance=1cm] (mu) {$\mu$};
    \node[draw=none, fill=none, right of=inv1, node distance=1cm] (sigma) {$\sigma$};
    \node[draw=none, fill=none, below of=inv1, node distance=1cm] (z) {$z$};
    \node[block, below of=z, node distance=1cm] (dense_2) {dense layer};
    \node[block, below of=dense_2, node distance=1cm] (deconv_1) {deconvolutional layer};
    \node [block, below of=deconv_1, node distance=1cm] (deconv_2) {deconvolutional layer};
    \node [draw=none, fill=none, below of=deconv_2, node distance=1cm] (out) {output image};
    % Draw edges
    \path [line] (inp) -- (conv_1);
    \path [line] (conv_1) -- (conv_2);
    \path [line] (conv_2) -- (dense_1);
    \path [line] (dense_1) -- (mu);
    \path [line] (dense_1) -- (sigma);
    \path [line] (mu) -- (z);
    \path [line] (sigma) -- (z);
    \path [line] (z) -- (dense_2);
    \path [line] (dense_2) -- (deconv_1);
    \path [line] (deconv_1) -- (deconv_2);
    \path [line] (deconv_2) -- (out);
  \end{tikzpicture}  
  \caption{Variational autoencoder architecture}
  \label{fig:neuralnet}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[scale=0.7, trim=0 0 0 0, clip]{costs.pdf}
  \caption{Value of loss function as model is trained.}
  \label{fig:training_costs}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{0_epochs.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{1_epochs.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{2_epochs.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{3_epochs.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{4_epochs.png}
  \caption{Autoencoder outputs for the first 10 images of the dataset after $n$ full runs through training dataset, from $n=0$ (top) to $n=4$ (bottom).}
  \label{fig:training_images}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{bag2sneaker.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{jumper2bag.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{trouser2jumper.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{tshirt2sneaker.png}
  \caption{Interpolations in latent space between selected images.}
  \label{fig:interpolations}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{bag2bag_extrahandle.png}
  \caption{Interpolations in latent space between a bag without and with a handle.}
  \label{fig:interpolation_handle}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{bags_default.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{bags_extrahandle.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{shoes_default.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{shoes_extrahandle.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{sandals_default.png}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{sandals_extrahandle.png}
  \caption{``Adding'' a handle to various images in latent space. The unmodified pictures are shown above those with an ``added'' handle.}
  \label{fig:extrahandles}
\end{figure}







	





\begin{thebibliography}{9}
          
\bibitem{notes}
  Webster, Kevin; Richemond, Pierre Harvey.
  \textit{Deep Learning}.
  Imperial College London, October 2018.
  https://www.deeplearningmathematics.com.

\bibitem{white}
  White, Tom.
  \textit{Sampling Generative Networks}.
  2016.
  
\bibitem{lamb}
  Lamb, Alex; Dumoulin, Vincent; Courville, Aaron.
  \textit{Discriminative Regularization for Generative Models}.
  2016.

\bibitem{frans}
  Frans, Kevin.
  \textit{Variational Autoencoders Explained}.
  2016.
  http://kvfrans.com/variational-autoencoders-explained/.

\bibitem{altosaar}
  Altosaar, Jaan.
  \textit{Tutorial - What is a Variational Autoencoder?}.
  2017.
  https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.

\bibitem{shafkat}
  Shafkaat, Irhum.
  \textit{Intuitively Understanding Variational Autoencoders}
  2018.
  https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
  
  
\end{thebibliography}

\end{document}          
