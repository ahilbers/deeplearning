\documentclass[]{article}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows}
\graphicspath{ {../figs/} }


% Title Page
\title{Industrial revolution 2.0: using variational autoencoders to produce images of clothing}
\author{Adriaan Hilbers}


\begin{document}
\maketitle




\section*{Abstract}
\label{sec:abstrct}

This document describes the use of variational autoencoders as generative models for images of articles of clothing. XXXXX.
This work was done as part of the \textit{Deep Learning} course at Imperial College London in Autumn 2018.




\section{Introduction}
\label{sec:introduction}

The terms \textit{machine learning} and \textit{artificial intelligence} are pervasive in recent media reporting. Many tasks until recently considered difficult for computers(e.g. character recognition) are now routine and can be performed to high accuracy with relatively modest algorithmic and computational complexities [\textit{source}].

Generative models are one specific class of machine learning algorithms that seek to produce representations (e.g. images) of objects that do not actually exist. There are various model architectures that can be employed, including generative adversarial neural networks, variational autoencoders or normalising flows \cite{notes}. Many of these approaches work by first \textit{training} a model on some existing data (such as images of objects) and then changing parameters inside the model. If set up correctly, adjusting these parameters induces meaningful changes in the outputs. For example, in \cite{white}, the author sets up a model such that one of its parameters corresponds to the rotation of a human face in an image.

In this report, we employ a \textit{variational autoencoder} to produce new images of clothing. The dataset and model architecture are introduced in sections XX and XX respectively.




\section{Dataset: Fashion MNIST}
\label{sec:dataset}

To train our model we use the \textit{Fashion MNIST} dataset\footnote{see https://github.com/zalandoresearch/fashion-mnist}. This dataset contains 70,000 images of articles of clothing, each given one of ten possible labels: t-shirt/top, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag or ankle boot. The data is split into 60,000 \textit{training} images and 10,000 \textit{testing} images. An overview of the images, by category, is shown in Figure \ref{fig:mnist_fashion}.

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_0.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_1.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_2.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_3.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_4.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_5.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_6.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_7.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_8.pdf}
  \includegraphics[scale=0.6, trim=90 0 70 0, clip]{clothes_9.pdf}
  \caption{100 samples images from the \textit{Fashion MNIST} dataset. Each row shows 10 examples from each clothing type: t-shirt/top, trousers, pullover, dress, coat, sandal, shirt, sneaker bag or ankle boot.}
  \label{fig:mnist_fashion}
\end{figure}

The \textit{Fashion MNIST} is designed to be a drop-in replacement for the traditional MNIST database of handwritten digits. They have the same image dimensions (28x28), with each image assigned a label between 0 and 9. However, the Fashion MNIST dataset is more complex, as differences between different categories (e.g. coat and pullover) is smaller than between different handwritten digits. Furthermore, differences \textit{within} categories (e.g. different sandals) can be much larger than for the handwritten digits.




\section{Variational Autoencoder: setup}
\label{sec:vae}

This section introduces the setup of the model and how it is trained. For sake of brevity, only a concise overview is provided. The setup of the model is shown in Figure \ref{fig:neuralnet}. Overall, a feedforward run of the variational autoencoder works as follows, as inspired by \cite{notes, frans, altosaar, shafkat}: 
\begin{itemize}
\item The raw input is an image encoded as a length 784 vector with the pixel brightnesses. 
\item This image is reshaped and fed through a convolutional neural network consisting of both convolutional and dense layers. This gives, for each image, two vectors: a mean$\mu$ and variance $\sigma$, each of dimension $n_z$. This phase is referred to as \textit{encoding}.
\item A random vector $z$ (of dimension $n_z$ is sampled from a normal distribution with mean $\mu$ and (diagional) covariance matrix parametrised by the diagonal values $\sigma$. This allows an optimiser to train through $\mu$ and $\sigma$ in what is known as the \textit{reparametrisation trick} \cite{notes, frans}.
\item $z$ is used as input in a deconvolutional neural network with a symmetric archicture as the encoder, returning a length 784 vector that can be interpreted as pixel values. This part of the neural network is known as the \textit{decoder}. 
\end{itemize}

The dimension of $n_z$ is typically much smaller than the original image. For a traditional autoencoder, the encoder maps straight to $z$ instead of $\mu$ and $\sigma$, and this part of the architecture is known as the \textit{bottleneck}. The goal of the autoencoder is to encode as much information as possible in a space of dimension $n_z$ by learning structures in the data. The goal of an autoencoder is to make the output image as ``close'' to the input image as possible with as low as possible value of $n_z$.

For a variational autoencoder, this is no longer the only goal. We also want to generate new outputs that do not exist in the training (or testing) dataset. For this reason, the space in which $z$ lives ($\mathbb{R}^N$) is called the \textit{latent space}. Sampling values of $z$ from this latent space and running them through a (trained) decoder should give new images.

For this to work, the distribution of encoded (noisy) $z$ values from the training dataset should be distributed in some meaningful way. We enforce this by adding a penalisation term to the loss function that measures the deviation of the distribution of $z$ values throughout the training dataset from a standard normal distribution $\mathcal{N}(0, 1)$. For this purpose, we use the Kullback-Leibler divergence $D_{KL}$.

The loss function is a follows:
\begin{align}
  \label{eq:loss}
  \mathcal{L}(\bm{\theta}, \bm{\phi}, \bm{x}^{(i)}) &= \frac{1}{2} \sum_{j=1}^d \left( 1 + \log{((\sigma_j^{(i)})^2)} - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2 \right) \\
  &+ \frac{1}{L} \sum_{l=1}^L \log{p_{\bm{\theta}}(\bm{x}^{(i)}|\bm{z}^{(i, l)})}
\end{align}

. 

\begin{figure}
  \centering
  % Define block styles
  \tikzstyle{block} = [rectangle, draw, fill=blue!20, 
      text width=12em, text centered, rounded corners, minimum height=1em]
  \tikzstyle{block_sm} = [rectangle, draw, fill=blue!20, 
      text width=4em, text centered, rounded corners, minimum height=1em]  
  \tikzstyle{line} = [draw, -latex']
    
  \begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node[draw=none,fill=none, node distance=1cm] (inp) {input image};
    \node[block, below of=inp, node distance=1cm] (conv_1) {convolutional layer};
    \node[block, below of=conv_1, node distance=1cm] (conv_2) {convolutional layer};
    \node[block, below of=conv_2, node distance=1cm] (dense_1) {dense layer};
    \node[draw=none, fill=none, below of=dense_1, node distance=1cm] (inv1) {};
    \node[draw=none, fill=none, left of=inv1, node distance=1cm] (mu) {$\mu$};
    \node[draw=none, fill=none, right of=inv1, node distance=1cm] (sigma) {$\sigma$};
    \node[draw=none, fill=none, below of=inv1, node distance=1cm] (z) {$z$};
    \node[block, below of=z, node distance=1cm] (dense_2) {dense layer};
    \node[block, below of=dense_2, node distance=1cm] (deconv_1) {deconvolutional layer};
    \node [block, below of=deconv_1, node distance=1cm] (deconv_2) {deconvolutional layer};
    \node [draw=none, fill=none, below of=deconv_2, node distance=1cm] (out) {output image};
    % Draw edges
    \path [line] (inp) -- (conv_1);
    \path [line] (conv_1) -- (conv_2);
    \path [line] (conv_2) -- (dense_1);
    \path [line] (dense_1) -- (mu);
    \path [line] (dense_1) -- (sigma);
    \path [line] (mu) -- (z);
    \path [line] (sigma) -- (z);
    \path [line] (z) -- (dense_2);
    \path [line] (dense_2) -- (deconv_1);
    \path [line] (deconv_1) -- (deconv_2);
    \path [line] (deconv_2) -- (out);
  \end{tikzpicture}  
  \caption{Variational autoencoder architecture}
  \label{fig:neuralnet}
\end{figure}









	





\begin{thebibliography}{9}
          
\bibitem{notes}
  Webster, Kevin; Richemond, Pierre Harvey.
  \textit{Deep Learning}.
  Imperial College London, October 2018.
  https://www.deeplearningmathematics.com.

\bibitem{white}
  White, Tom.
  \textit{Sampling Generative Networks}.
  2016.
  
\bibitem{lamb}
  Lamb, Alex; Dumoulin, Vincent; Courville, Aaron.
  \textit{Discriminative Regularization for Generative Models}.
  2016.

\bibitem{frans}
  Frans, Kevin.
  \textit{Variational Autoencoders Explained}.
  2016.
  http://kvfrans.com/variational-autoencoders-explained/.

\bibitem{altosaar}
  Altosaar, Jaan.
  \textit{Tutorial - What is a Variational Autoencoder?}.
  2017.
  https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.

\bibitem{shafkat}
  Shafkaat, Irhum.
  \textit{Intuitively Understanding Variational Autoencoders}
  2018.
  https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
  
  
\end{thebibliography}

\end{document}          
