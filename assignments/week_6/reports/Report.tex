\documentclass[]{article}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows}
\usetikzlibrary{calc,arrows,decorations.pathmorphing,backgrounds,fit,positioning,shapes.symbols,chains}
\usetikzlibrary{decorations.pathreplacing}
\graphicspath{ {../figs/} }


% Title Page
\title{\vspace{-8ex} Industrial revolution 2.0: using variational autoencoders to produce images of clothing}
\author{Adriaan Hilbers}
\date{December 2018}


\begin{document}
\maketitle




\section*{Abstract}
\label{sec:abstrct}

This document describes the use of variational autoencoders as generative models for images of clothing, using the \textit{Fashion MNIST} dataset. This work was done as part of the \textit{Deep Learning} course at Imperial College London in Autumn 2018.




\section{Introduction}
\label{sec:introduction}

\hspace{\parindent} In the last few decades, machine learning techniques have achieved considerable success in many tasks previously considered difficult for a computer. \textit{Generative models} seek to produce new (digital) objects, such as images, without human assistance. They are typically \textit{trained} on ``real'' data such as existing images or audio fragments to learn their structures, which are imitated to form new examples. Various generative frameworks exist, including generative adversarial neural networks, variational autoencoders and normalising flows \cite{notes}. 

In this report, we employ a variational autoencoder to produce new images of clothing. The dataset and model architecture are introduced in sections \ref{sec:dataset} and \ref{sec:vae} respectively. Section \ref{sec:training} describes its training, while Section \ref{sec:generating} describes its use to generate new images of clothing. Section \ref{sec:discussion} discusses the results and recommends possible extensions.

Code for this assignment, using Python and Tensorflow, is publicly available at \texttt{github.com/ahilbers/deeplearning/assignments/week\_6}. 




\section{Dataset: Fashion MNIST}
\label{sec:dataset}

\hspace{\parindent} We use the \textit{Fashion MNIST} dataset\footnote{https://github.com/zalandoresearch/fashion-mnist} to train our model. It contains 70,000 images of articles of clothing, classified into one of 10 categories. The data is split into 60,000 \textit{training} images and 10,000 \textit{testing} images. An overview of the images, by category, is shown in Figure \ref{fig:mnist_fashion}.

\begin{figure}
  \centering
  T-shirt/top
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_0.pdf}
  Trousers
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_1.pdf}
  Pullover
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_2.pdf}
  Dress
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_3.pdf}
  Coat
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_4.pdf}
  Sandal
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_5.pdf}
  Shirt
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_6.pdf}
  Sneaker
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_7.pdf}
  Bag
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_8.pdf}
  Ankle boot
  \includegraphics[scale=0.6, trim=90 0 70 5, clip]{clothes_9.pdf}
  \caption{Samples images from the \textit{Fashion MNIST} dataset.}
  \label{fig:mnist_fashion}
\end{figure}

The \textit{Fashion MNIST} is a drop-in replacement for the traditional MNIST database of handwritten digits. However, it is more challenging for machine learning algorithms since the inter-category variations are smaller and the intra-category variations larger. 




\section{Variational autoencoder: setup}
\label{sec:vae}

\hspace{\parindent} In this section we introduce the variational autoencoder's architecture and how it is trained. It is inspired by \cite{notes, frans, altosaar, shafkat}, and consists of two parts: an \textit{encoder}, mapping each image into the \textit{latent space}, and a \textit{decoder}. A high level overview is as follows:
\begin{itemize}
\item Represent an image by $\bm{x}^{(i)}$, the length 784 vector of its pixel brightnesses.
\item Encode this image into a mean vector $\bm{\mu}^{(i)}$ and variance vector $\bm{\sigma}^{(i)}$ with dimension $n_z$. Typically, this dimension is much smaller than that of $\bm{x}^{(i)}$. 
\item Sample a vector $\bm{z}^{(i)}$ from $\mathcal{N}(\bm{\mu}^{(i)}, \bm{\sigma}^{(i)})$ with diagional covariance matrix. In reality, a \textit{reparametrisation trick} (see e.g. \cite{notes}) is performed to ensure the model is trainable, but this does not change the results of a feedforward run through the autoencoder). The density of $\bm{z}^{(i)}$ given $\bm{x}^{(i)}$ is denoted by $q_{\phi}(\bm{z}^{(i)}|\bm{x}^{(i)})$, where $\phi$ represents all parameters in the encoder.
\item Decode $\bm{z}^{(i)}$ back into image space using a decoder $p_{\theta}(\bm{x}^{(i)}|\bm{z}^{(i)})$, where $\theta$ represents all parameters in the decoder.
\end{itemize}
Figure \ref{fig:neuralnet} provides a graphical overview of the encoder and decoder with all the neural network layers. The encoder and decoder are symmetric, so that the number of dimensions after e.g. the first convolutional layer in the encoder is equal to that before the last one in the decoder. The trainable parameters are $\phi$ (encoder) and $\theta$ (decoder).  

\begin{figure}
  \centering
  % Define block styles
  \tikzstyle{block} = [rectangle, draw, fill=blue!20, 
      text width=12em, text centered, rounded corners, minimum height=1em]
  \tikzstyle{block_sm} = [rectangle, draw, fill=blue!20, 
      text width=4em, text centered, rounded corners, minimum height=1em]  
  \tikzstyle{line} = [draw, -latex']
    
  \begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node[draw=none,fill=none, node distance=1cm] (inp) {input image $\bm{x}^{(i)}$};
    \node[block, below of=inp, node distance=1cm] (conv_1) {convolutional layer};
    \node[block, below of=conv_1, node distance=1cm] (conv_2) {convolutional layer};
    \node[block, below of=conv_2, node distance=1cm] (dense_1) {dense layer};
    \node[block, below of=dense_1, node distance=1cm] (dense_2) {dense layer};
    \node[draw=none, fill=none, below of=dense_2, node distance=1cm] (inv1) {};
    \node[draw=none, fill=none, left of=inv1, node distance=1cm] (mu) {$\bm{\mu}^{(i)}$};
    \node[draw=none, fill=none, right of=inv1, node distance=1cm] (sigma) {$\bm{\sigma}^{(i)}$};
    \node[draw=none, fill=none, below of=inv1, node distance=1cm] (z) {$\bm{z}^{(i)} \sim \mathcal{N}(\bm{\mu}^{(i)}, \bm{\sigma}^{(i)}) = q_\phi(\bm{z}^{(i)} | \bm{x}^{(i)})$};
    \node[block, below of=z, node distance=1cm] (dense_3) {dense layer};
    \node[block, below of=dense_3, node distance=1cm] (dense_4) {dense layer};
    \node[block, below of=dense_4, node distance=1cm] (deconv_1) {deconvolutional layer};
    \node [block, below of=deconv_1, node distance=1cm] (deconv_2) {deconvolutional layer};
    \node [draw=none, fill=none, below of=deconv_2, node distance=1cm] (out) {output image $p_{\theta}(\bm{x}^{(i)}|\bm{z}^{(i)})$};
    % Draw edges
    \path [line] (inp) -- (conv_1);
    \path [line] (conv_1) -- (conv_2);
    \path [line] (conv_2) -- (dense_1);
    \path [line] (dense_1) -- (dense_2);
    \path [line] (dense_2) -- (mu);
    \path [line] (dense_2) -- (sigma);
    \path [line] (mu) -- (z);
    \path [line] (sigma) -- (z);
    \path [line] (z) -- (dense_3);
    \path [line] (dense_3) -- (dense_4);
    \path [line] (dense_4) -- (deconv_1);
    \path [line] (deconv_1) -- (deconv_2);
    \path [line] (deconv_2) -- (out);

    \draw [decorate,decoration={brace,amplitude=10pt},xshift=0pt,yshift=0pt]
    (-3,-4.2)--(-3,-0.8)  node [black,midway,xshift=-0.6cm,yshift=0cm] {Encoder};
    \draw [decorate,decoration={brace,amplitude=10pt},xshift=0pt,yshift=0pt]
    (-3,-6.4)--(-3,-4.6)  node [black,midway,xshift=-0.6cm,yshift=0cm] {``Bottleneck''};
    \draw [decorate,decoration={brace,amplitude=10pt},xshift=0pt,yshift=0pt]
    (-3,-10.2)--(-3,-6.8)  node [black,midway,xshift=-0.6cm,yshift=0cm] {Decoder};
  \end{tikzpicture}  
  \caption{Variational autoencoder architecture. The input image $\bm{x}^{(i)}$ (a length 784 vector) is ``encoded'' into values of $\mu$ and $\theta$, each with dimension $n_z$. A noisy value of $\bm{z}^{(i)}$ (with dimension $n_z$) is drawn from a normal distribution with these parameters (using the reparametrisation trick to ensure the model is trainable). $\bm{z}^{(i)}$ is subsequently ``decoded'' back into a vector of length 784.}
  \label{fig:neuralnet}
\end{figure}

In a traditional autoencoder, the loss function is some reconstruction error between $p_{\sigma}(\bm{x}^{(i)}|\bm{z}^{(i)})$ and the input $\bm{x}^{(i)}$. In a variational autoencoder, we want to generate new images. This is usally done by decoding specific values of $\bm{z}$ from the latent space. For this reason, we need to ensure that the latent space is ``meaningful'', in the sense that a $\bm{z}$ value halfway between a shoe and trousers is some pictorial intermediate of the two.

This  is usually done by enforcing that the distribution of (noisy) $\bm{z}^{(i)}$ values throughout the training dataset is close to a unit normal. This can be achieved by adding a penalisation term to the loss function. A convenient choice is the Kullback-Leibler divergence between the distribution of $\bm{z}^{(i)}$ values and $\mathcal{N}(0, 1)$, since this term can be expressed analytically. The loss function we use to train is hence: 
\begin{equation}
  \mathcal{L}(\bm{\theta}, \bm{\phi}) = \sum_{i=1}^{N_i} \mathcal{L}(\bm{\theta}, \bm{\phi}, \bm{x}^{(i)})
\end{equation}
where the index $i$ corresponds to the the (training) images and
\begin{multline}
  \label{eq:loss}
  \mathcal{L}(\bm{\theta}, \bm{\phi}, \bm{x}^{(i)}) = \frac{1}{2} \sum_{j=1}^{n_z} \left( 1 + \log{((\sigma_j^{(i)})^2)} - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2 \right) \\
  + \frac{1}{L} \sum_{l=1}^L \log{p_{\bm{\theta}}(\bm{x}^{(i)}|\bm{z}^{(i, l)})}
\end{multline}
with $\mu_j^{(i)}$ the $j$th component of $\bm{\mu}^{(i)}$, $\sigma_j^{(i)}$ defined the same way and $\bm{z}^{(i, l)}$ one of $L$ samples randomly drawn from $\mathcal{N}(\bm{\mu}^{(i)}, \bm{\sigma}^{(i)})$. The first term is an analytic expression for the Kullback-Leibler divergence of the $\bm{z}$ space encoded by $q_\phi$ and the second measures the reconstruction error. A traditional autoencoder would only include the second term. Note that the encoder parameters $\phi$ are implicitly present in the the second term through $\bm{z}^{(i, l)}$. 




\section{Training}
\label{sec:training}

\begin{figure}
  \centering
  \includegraphics[scale=0.7, trim=0 0 0 0, clip]{costs.pdf}
  \caption{Value of loss function as model is trained.}
  \label{fig:training_costs}
\end{figure}

\begin{figure}
  \centering
  0 full runs through training dataset (epochs)
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{0_epochs.png} \\
  1 full run through training dataset (epoch)
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{1_epochs.png} \\
  2 full runs through training dataset (epochs)
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{2_epochs.png} \\
  3 full runs through training dataset (epochs)
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{3_epochs.png} \\
  4 full runs through training dataset (epochs)
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{4_epochs.png} \\
  Original images
  \includegraphics[scale=0.6, trim=82 0 0 0, clip]{benchmark.pdf}
  \caption{Autoencoder outputs for the first 10 images of the dataset after $n$ full runs through training dataset (epochs), from $n=0$ (top) to $n=4$ (bottom). The last row shows the corresponding original images.}
  \label{fig:training_images}
\end{figure}

\hspace{\parindent} To train the model, we use Tensorflow's inbuilt \textit{Adam} optimiser, along with the training/testing split provided by the dataset. We train in batches of size 100, up to five full runs through the training dataset (epochs). The training and testing errors are shown in Figure \ref{fig:training_costs}, with the expected results. Since the value of the cost/loss function is not easily intepretable, we also plot 10 reconstructions as a function of the number of epochs of training. The original images are shown in the last row as comparison. Note that the autoencoder does well in reconstructing the overall shape of the images, but blurs them considerably and, for this reason, fails to capture more detailed features such as the designs on t-shirts. This is often seen when minimising a loss function without extra adjustments, and is a problem common in (variational) autoencoders \cite{lamb, frans}. 




\section{Generating images}
\label{sec:generating}

\hspace{\parindent} The variational autoencoder was chosen for its potential as a \textit{generative} model, so we want not just to recreate available images, but also to create new ones. The idea is that the latent space in which $\bm{z}$ lives has some meaning, and we can create images with custom features by decoding suitable $\bm{z}$ values. In this section, we explore to what extent we have made this possible.


\subsection{Interpolating}
\label{sec:generating:interpolating}

\begin{figure}
  \centering
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{bag2sneaker.png}
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{jumper2bag.png}
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{trouser2jumper.png}
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{tshirt2sneaker.png}
  \caption{Interpolations in latent space between selected images.}
  \label{fig:interpolations}
\end{figure}

\hspace{\parindent} We start by interpolating images in latent space. Interpolations in the original 784-dimensional image are superpositions of pixel brighntesses and do not provide images resembling real clothing. We hope that interpolating in latent space returns a continous mapping from one image to another, where each intermediate image looks like an item of clothing.

Interpolations in latent space between various images are shown in Figure \ref{fig:interpolations}. The results are roughly what was expected: an image ``morphs'' continuously into another one. Interestingly, there are sometimes intermediate steps where the interpolation ``passes through'' another item. For example, in the last row of Figure \ref{fig:interpolations}, a pullover becomes a bag and only then a sneaker.


\subsection{Adding custom features}
\label{sec:generating:features}

\begin{figure}
  \centering
  Adding a handle to bags
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{bags_default.png}
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{bags_extrahandle.png} \\
  Adding a handle to sneakers
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{shoes_default.png}
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{shoes_extrahandle.png} \\
  Adding a handle to sandals
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{sandals_default.png}
  \includegraphics[scale=0.6, trim=28 0 10 0, clip]{sandals_extrahandle.png}
  \caption{``Adding'' a handle to various images in latent space. The unmodified pictures are shown above those with an ``added'' handle.}
  \label{fig:extrahandles}
\end{figure}

\hspace{\parindent} Interpolations provide only a limited form of generative power; we would also like to create images with custom features. Suppose, for example, we want to generate an image of a sneaker with a handle added to it. We can do this via
\begin{align}
  \label{eq:sneaker_handle}
  \text{sneaker with handle} &= \text{sneaker} + \text{handle} \\
  &= \text{sneaker} + (\text{bag with handle} - \text{bag without handle})
\end{align}
where all addition occurs in latent space.

We use this approach to ``add'' handles to various articles of clothing. The results are displayed in Figure \ref{fig:extrahandles}. The first two rows are bags, with the original (autoencoded) images above and those with an added handle below. The approach does not work perfectly: it tends to add features that somewhat resemble a handle but would probably not be recognised as such by a human. For example, for the first bag (left top image of Figure \ref{fig:extrahandles}), the bag gets a bulge where a handle would be, but there is no ``loop''. For bags already containing a handle, it tends to be come slightly thicker. The only bag for which a legitimate handle is added is the 9th (top row, second from right). However, it and the image below it are the images the difference was taken over to isolate the handle in the first place. 

For sneakers (third and fourth row), the resultant images do appear to have a handle, but the sneaker shape has been destroyed, so that we are left with blob-like shapes instead of shoes. For sandals, it is very difficult to interpret the bottom row as images. However, interestingly, the last image (right bottom) looks like a coat, evidencing the complex structure in which $\bm{z}$ values live.




\section{Discussion}
\label{sec:discussion}

\hspace{\parindent} Most interesting behavior is related to the structure of the latent space and this is the most obvious area of improvement. If we can make latent space ``meaningful'', we may have a powerful generative tool. However, the success of this may require more careful tuning. For example, the Kullback-Leibler divergence as a penalisation term can be viewed as somewhat simplistic, and a more sophisticated loss function may give more control over the distribution of $\bm{z}$ values. We can also study the latent space in more detail in the hopes of understanding its structure better.

Another drawback is the considerable blur on (re)constructed images. This blur remains even when the ``obvious'' improvements of more hidden layers, nodes or filters are applied, and is a feature of simple reconstruction error loss functions. Some papers (e.g. \cite{lamb}) recommend adjustments to alleviate this issue.




\begin{thebibliography}{9}
          
\bibitem{notes}
  Webster, Kevin; Richemond, Pierre Harvey.
  \textit{Deep Learning}.
  Imperial College London, October 2018.
  https://www.deeplearningmathematics.com.

\bibitem{white}
  White, Tom.
  \textit{Sampling Generative Networks}.
  2016.
  
\bibitem{lamb}
  Lamb, Alex; Dumoulin, Vincent; Courville, Aaron.
  \textit{Discriminative Regularization for Generative Models}.
  2016.

\bibitem{frans}
  Frans, Kevin.
  \textit{Variational Autoencoders Explained}.
  2016.
  http://kvfrans.com/variational-autoencoders-explained/.

\bibitem{altosaar}
  Altosaar, Jaan.
  \textit{Tutorial - What is a Variational Autoencoder?}.
  2017.
  https://jaan.io/what-is-variational-autoencoder-vae-tutorial/.

\bibitem{shafkat}
  Shafkaat, Irhum.
  \textit{Intuitively Understanding Variational Autoencoders}
  2018.
  https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
  
  
\end{thebibliography}

\end{document}          
